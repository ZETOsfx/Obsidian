## Случайный поиск. Градиентный метод

**Градиентный метод** - метод нахождения минимального значения функции потерь (существует множество видов этой функции). Минимизация любой функции означает поиск самой глубокой впадины в этой функции.

Градиентный спуск нужен для минимизации функции потерь.

*Суть алгоритма*: процесс получения наименьшего значения ошибки. Аналогично это можно рассматривать как спуск во впадину в попытке найти золото на дне ущелья (самое низкое значение ошибки).

![градиентный спуск пример](https://neurohive.io/wp-content/uploads/2018/11/minimum-funkcii-metod-gradientnogo-spuska-570x320.png)

### Различные типы градиентного спуска

Существует 3 варианта градиентного спуска:

1. **Мini-batch**: тут, вместо поочерёдного перебора всех примеров обучения и произведения необходимых вычислений для каждого из них, мы производим вычисления для n примеров обучения сразу. Этот выбор подходит для очень больших наборов данных.

2. **Стохастический градиентный спуск**: в этом случае вместо перебора и использования всего набора примеров обучения мы применяем подход “используй только один”. Здесь нужно отметить несколько моментов:

- Набор примеров обучения необходимо перемешать перед каждым его проходом в ГС, чтобы перебирать их каждый раз в случайном порядке.
- Поскольку каждый раз используется только один пример обучения, то ваш путь к локальному минимуму будет очень неоптимальным;
- С каждой итерацией ГС вам нужно перемешать набор обучения и выбрать случайный пример обучения;
- Поскольку вы используете только один пример обучения, ваш путь к локальным минимумам будет очень шумным.

3. **Серия ГС**: это то, о чем написано в предыдущих разделах. Цикл на каждом примере обучения.

![локальный минимум градиентный спуск](https://neurohive.io/wp-content/uploads/2018/11/lokalnyi-minimum-gradientnyi-spusk.png)

Пример кода градиентного спуска на Python.  
```python
def train(X, y, W, B, alpha, max_iters):
	'''
	Performs GD on all training examples,
	X: Training data set,
	y: Labels for training data,
	W: Weights vector,
	B: Bias variable,
	alpha: The learning rate,
	max_iters: Maximum GD iterations.
	'''
	dW = 0 # Weights gradient accumulator
	dB = 0 # Bias gradient accumulator
	m = X.shape[0] # No. of training examples
	for i in range(max_iters):
		dW = 0 # Reseting the accumulators
		dB = 0
		for j in range(m):
			# 1. Iterate over all examples,
			# 2. Compute gradients of the weights and biases in w_ and b_grad,
			# 3. Update dW by adding w_grad and dB by adding b_grad,
			W = W - alpha * (dW / m) # Update the weights
			B = B - alpha * (dB / m) # Update the bias
```

---
## Эволюционный поиск

Для эволюционного поиска, в отличие от других методов поиска решения, необходимы четкие ограничения на входные параметры, а поиск проходит по всему объему сразу. 

На каждом шаге из множества точек выбирается группа "хороших" по установленным критериям.

---

Next door: [[АС - Лекция 4]]

